chunk
"How to Prompt LLMs for Text-to-SQL: A Study in Zero-shot,
Single-domain, and Cross-domain Settings

Shuaichen Chang and Eric Fosler-Lussier
The Ohio State University
{chang.1692, fosler-lussier.1}@osu.edu

3
2
0
2

v
o
N
7
2

]
L
C
.
s
c
[

3
v
3
5
8
1
1
.
5
0
3
2
:
v
i
X
r
a

Abstract

Large language models (LLMs) with in-context
learning have demonstrated remarkable ca-
pability in the text-to-SQL task. Previous
research has prompted LLMs with various
demonstration-retrieval strategies and interme-
diate reasoning steps to enhance the perfor-
mance of LLMs. However, those works of-
ten employ varied strategies when constructing
the prompt text for text-to-SQL inputs, such as
databases and demonstration examples. This
leads to a lack of comparability in both the
prompt constructions and their primary contri-
butions. Furthermore, selecting an effective
prompt construction has emerged as a persis-
tent problem for future research. To address
this limitation, we comprehensively investigate
the impact of prompt constructions across var-
ious settings and provide insights into prompt
constructions for future text-to-SQL studies. 1

1

Introduction"
"1

Introduction

Text-to-SQL models enable users
to query
databases using natural language questions (NLQs)
without having to develop the underlying SQL
query. Over the past few decades, neural models
with supervised learning have achieved impressive
performance on the text-to-SQL task, which are
usually trained on a large training set and then eval-
uated on test examples (Wang et al., 2019; Yu et al.,
2021; Rubin and Berant, 2021; Scholak et al., 2021;
Gan et al., 2021; Li et al., 2023a).

Recently, large language models (LLMs) have
demonstrated strong capabilities for in-context
learning on many language understanding and gen-
eration tasks (Brown et al., 2020; Chen et al.,
2021a; Chowdhery et al., 2022), including on the
text-to-SQL task (Rajkumar et al., 2022; Chang
Instead of train-
et al., 2023; Liu et al., 2023).
ing a text-to-SQL model on a large training set,

1The code for the paper is available at https://github.

com/shuaichenchang/prompt-text-to-sql.

Database
CREATE TABLE Highschooler (
ID int primary key ,
name text ,
grade int
);
/*
3 example rows :
SELECT * FROM Highschooler LIMIT 3;
ID
1510
1689
1381
*/

Jordan
Gabriel
Tiffany

grade
9

name

9
9

Task Instruction

-- Using valid SQLite , answer the following
questions for the tables provided above .

Demonstration

Question : What is Kyle 's id ?
SELECT ID FROM Highschooler WHERE name = ""
Kyle "";

Test Question

Question : How many high schoolers are there ?
SELECT"
"Demonstration

Question : What is Kyle 's id ?
SELECT ID FROM Highschooler WHERE name = ""
Kyle "";

Test Question

Question : How many high schoolers are there ?
SELECT

Figure 1: An example of prompt text for 1-shot single-
domain text-to-SQL using a snippet of the database
Network_1 with a question from the Spider dataset (Yu
et al., 2018).

in-context learning allows LLMs to convert a test
NLQ into a SQL query using a prompt text. This
prompt text includes essential components such
as the test database and question. These are
accompanied by zero or a few demonstrations:
NLQ-SQL pairs corresponding to either the test
database (single-domain) or different databases
(cross-domain). Figure 1 provides an example of a
prompt text for a one-shot single-domain task.

Previous research has augmented the text-to-
SQL capability of LLMs with demonstration-
retrieval strategies (Poesia et al., 2022; Shi et al.,
2022), intermediate reasoning steps (Cheng et al.,
2022; Chen et al., 2023; Pourreza and Rafiei, 2023),
and self-debugging ability (Chen et al., 2023; Pour-
reza and Rafiei, 2023). However, those studies of-
ten employ different prompt strategies that include
various key components of text-to-SQL: database

schema and content, and demonstration examples.
The difference in prompt constructions makes it
difficult to directly compare two studies on their
main contribution, and the outcomes of different
studies may change based on future revelations in
prompt engineering."
"In this paper, we evaluate various strategies for
prompt construction in three commonly employed
text-to-SQL settings: zero-shot, single-domain,
and cross-domain. We assess LLMs on text-to-
SQL, considering various database prompt con-
structions in all three settings. Additionally, in the
cross-domain scenario, we investigate the strategy
for constructing demonstrations. Through our eval-
uation, we aim to gain insights into the effective-
ness of these prompt construction strategies. Our
findings can be summarized as follows:

• Table relationship and table content play a crucial
role in effectively prompting LLMs. However,
it is essential to carefully consider their repre-
sentation in the prompt, as LLMs are sensitive
to the specific presentation in the zero-shot and
cross-domain settings.

• In-domain demonstration examples can mitigate
LLMs’ sensitivity to different representations of
database knowledge but they cannot replace table
content knowledge.

• The length of the prompt has a significant impact
on the LLMs’ performance in the cross-domain
setting. We discovered a preferred prompt length
that leads to improved performance.

2

In-context Learning for Text-to-SQL"
"• The length of the prompt has a significant impact
on the LLMs’ performance in the cross-domain
setting. We discovered a preferred prompt length
that leads to improved performance.

2

In-context Learning for Text-to-SQL

In the text-to-SQL task, a database and a natural
language question (NLQ) are provided as input
for generating an output SQL query. Traditional
supervised learning approaches train models on
specific text-to-SQL datasets. However, in-context
learning allows pretrained large language models
(LLMs) to perform text-to-SQL by providing either
zero or a few training examples (NLQ-SQL pairs)
as demonstrations. This section introduces three
widely used settings for in-context learning in text-
to-SQL. Prompt examples in these settings can be
found in Appendix A.1.

Zero-shot Text-to-SQL This setting evaluates
the text-to-SQL capability of pretrained LLMs to
directly infer the NLQ-SQL relationship from a
table without any demonstration examples. The
input includes a task instruction and a test question

with its corresponding database. Zero-shot text-
to-SQL is used to directly assess the text-to-SQL
capability of LLMs (Rajkumar et al., 2022; Chang
et al., 2023; Liu et al., 2023)."
"with its corresponding database. Zero-shot text-
to-SQL is used to directly assess the text-to-SQL
capability of LLMs (Rajkumar et al., 2022; Chang
et al., 2023; Liu et al., 2023).

Single-domain Few-shot Text-to-SQL This set-
ting is designed for applications or domains where
it is easy to construct examples, such as booking
flights (Price, 1990; Dahl et al., 1994) and querying
geographic information (Zelle and Mooney, 1996).
It tests the ability of LLMs to adapt with a few
in-domain demonstration examples, which are col-
lected from the same database as the test question.
The goal is to evaluate how well the LLMs can per-
form text-to-SQL with minimal in-domain training
data (Rajkumar et al., 2022).

Cross-domain Few-shot Text-to-SQL This set-
ting evaluates the generalization capability of mod-
els to new domains by learning from out-of-domain
demonstrations. In this scenario, the demonstra-
tion NLQ-SQL pairs correspond to one or multiple
demonstration databases that are different from the
test database. Cross-domain few-shot text-to-SQL
assesses how well LLMs can apply their learned
knowledge from demonstrations to new databases
(Poesia et al., 2022; Chen et al., 2023).

3 Prompt Construction"
"3 Prompt Construction

A text-to-SQL prompt typically comprises four
components: a task instruction, a test database,
a test NLQ, and optional demonstrations, as il-
lustrated in Figure 1. While the task instruction
and test NLQ are easily presented in natural lan-
guage, there are various strategies for representing
the databases and incorporating demonstrations. In
this section, we explore different prompt construc-
tions for databases and demonstrations.

3.1 Database Prompt

A relational database consists of the database
schema and database content. The database schema
encompasses the schemas (headers) of tables and
the relationship among tables, and database content
refers to the data stored in the tables.

Database Schema Figure 2 illustrates various
prompt constructions for the database schema
that have been utilized in previous studies: (1)
Table(Columns) (Liu et al., 2023) lists each table
along with its columns inside parentheses to repre-
sent the table schemas; (2) Columns=[] (Pourreza
and Rafiei, 2023) represents each table along with

Table(Columns) (Liu et al., 2023)
Highschooler (ID , name , grade );
Friend ( student_id , friend_id );

Columns=[] (Pourreza and Rafiei, 2023)
Table Highschooler , Columns = [ID , name ,
grade ];
Table Friend , Columns = [ student_id ,
friend_id ];

+FK (Pourreza and Rafiei, 2023)
Foreign_keys = [ Friend . student_id =
Highschooler .ID , Friend . friend_id =
Highschooler . ID ];

CreateTable (Rajkumar et al., 2022)"
"+FK (Pourreza and Rafiei, 2023)
Foreign_keys = [ Friend . student_id =
Highschooler .ID , Friend . friend_id =
Highschooler . ID ];

CreateTable (Rajkumar et al., 2022)

CREATE TABLE Highschooler (
ID int primary key ,
name text ,
grade int
);
CREATE TABLE Friend (
student_id int ,
friend_id int ,
primary key ( student_id , friend_id ) ,
foreign key ( student_id ) references
Highschooler ( ID ) ,
foreign key ( friend_id ) references
Highschooler ( ID )
);

Figure 2: Examples of the different database schema
constructions for a snippet of database Network_1 in
Spider.

a list of its columns using an equation-like notation;
(3) +ForeignKey (Pourreza and Rafiei, 2023) fur-
ther adds foreign keys to indicate the relationships
between tables; (4) CreateTable (Rajkumar et al.,
2022) employed the “Create Table” statement to
display the table schemas and relationships.

To ensure consistency in the prompt text and
accommodate the case-insensitivity of SQL key-
words and the database schema, we unify the space
and line break in the prompt text and convert all
words to lowercase, except for the database content.
This normalization process helps to standardize the
prompt text. An example is shown in Figure 4."
"Database content Previous research shows that
being aware of database content can improve model
performance by exposing models to the specific
format of values in each column (Wang et al., 2019;
Lin et al., 2020; Scholak et al., 2021; Rajkumar
et al., 2022). For instance, the phrase “American
student” could be converted to “WHERE country
= ‘USA’” or “WHERE country = ‘The United
States of America’” depending on the contents
of the country column.

Figure 3 summarizes different approaches used
to construct prompts for showcasing the content
of a database. (1) InsertRow (Chen et al., 2023):

InsertRow (Chen et al., 2023)
INSERT INTO Highschooler (ID , name , grade )
VALUES (1510 , "" Jordan "" , 9) ;
INSERT INTO Highschooler (ID , name , grade )
VALUES (1689 , "" Gabriel "" , 9) ;
INSERT INTO Highschooler (ID , name , grade )
VALUES (1381 , "" Tiffany "" , 9) ;

SelectRow (Rajkumar et al., 2022)
/*
3 example rows :
SELECT * FROM Highschooler LIMIT 3;
ID
1510
1689
1381
*/

Jordan
Gabriel
Tiffany

grade
9

name

9
9

SelectCol (Ours)
/*
Columns in Highschooler and 3 distinct
examples in each column :
ID : 1025 , 1101 , 1247
name : "" Jordan "", "" Gabriel "", "" Tiffany ""
grade : 9, 10 , 11
*/

Figure 3: Examples of the different database content
constructions for showing 3 cell values in each column
for the Highschool table in Figure 2."
"Figure 3: Examples of the different database content
constructions for showing 3 cell values in each column
for the Highschool table in Figure 2.

This method displays R rows of each table by utiliz-
ing R “INSERT INTO” statements. (2) SelectRow
(Rajkumar et al., 2022): This approach employs the
“SELECT * FROM Table LIMIT R” query to display
the first R rows of each table. (3) SelectCol: In-
stead of presenting table content in a row-wise man-
ner, an alternative method is to use a column-wise
format. As there may be duplicated content across
different rows, presenting the content column-wise
ensures the provision of distinct values within each
column to expose LLMs to a broader range of
content. We propose using the query “SELECT
DISTINCT [Column] FROM [Table] LIMIT R”
to list R distinct cell values in each column.

3.2 Demonstration Prompt"
"3.2 Demonstration Prompt

In few-shot settings, LLMs are provided with
demonstrations within the prompt text.
In the
single-domain few-shot setting, we incorporate a
few pairs of NLQs and SQLs as demonstrations
inserted between the test database and question,
following previous work (Rajkumar et al., 2022).
In the cross-domain few-shot setting, we use both
out-of-domain NLQ-SQL pairs (demonstration ex-
amples) and corresponding databases (demonstra-
tion databases) placed before the test database and
question. Prior research in the N -shot setting either
uses one demonstration database with N examples
(Pourreza and Rafiei, 2023) or employs N demon-
stration databases, each with a single NLQ-SQL

Unnormalized database and SQL

-- Database Schema
CREATE TABLE Highschooler (

ID int primary key ,
name text ,
grade int );

-- SQL Query
SELECT count ( * ) FROM Highschooler WHERE
Name = "" Kyle "";

Normalized database and SQL

-- Database Schema
create table highschooler (
id int primary key ,
name text ,
grade int
);

-- SQL Query
select count (*) from highschooler where name
= ' Kyle ';

Figure 4: An example of the normalization for database
and SQL prompts.

pair (Poesia et al., 2022; Chen et al., 2023). In con-
trast, we consider a more general scenario where
the demonstrations comprise M databases, each
with K NLQ-SQL pairs, with M × K = N . We
list the examples of 4-shot single-domain and cross-
domain demonstrations in Appendix A.1."
"Additionally, we normalize demonstration SQL
queries by first parsing the SQL queries and unify-
ing their format, such as using lowercase for SQL
keywords and database schema and unifying the
space around punctuation. Figure 4 provides an
example of SQL normalization.

4 Experiments

Data & Evaluation For our experiments, we uti-
lize the Spider dataset (Yu et al., 2018), a cross-
domain benchmark for the text-to-SQL task. We
conduct our experiments on the development set of
Spider (Spider-dev) as the test set is not publicly
available. Spider-dev consists of 20 databases with
1034 pairs of NLQ and SQL in total. We evaluate
models with execution accuracy (EX) which com-
pares the execution results of a predicted SQL and
a gold SQL.

In the cross-domain setting, we use the train-
ing set of Spider to select demonstration exam-
ples. As a few databases contain long schema that
may cause the prompt to exceed the token limits of
LLMs, we only use the databases with fewer than
1000 tokens when constructing the CreateTable
prompt. This results in a total of 130 databases
being used as demonstration databases in the cross-
domain setting.

Models We used GPT-3 Codex (Chen et al.,
2021a) and ChatGPT due to their demonstrated
performance and prevalence in the field.2"
"Models We used GPT-3 Codex (Chen et al.,
2021a) and ChatGPT due to their demonstrated
performance and prevalence in the field.2

Experiment Setup For the zero-shot setting, we
construct each prompt text with a task instruction,
a test database, and a test question. We include
R = 3 table rows in the database prompt, which
has been discovered as the optimal number in previ-
ous work (Rajkumar et al., 2022). For the few-shot
settings, we incorporate N demonstration exam-
ples in addition to the zero-shot prompt text.

In the single-domain text-to-SQL scenario, we
use a leave-one-out split, as some databases in
Spider-dev contain a small number of examples.
When evaluating one example, we regard all other
examples from the same database as the training
set and randomly retrieve N examples from them.
Since Spider contains multiple NLQs correspond-
ing to the same SQL query, we require that the train-
ing set does not contain examples with the same
SQL template as the test example, again following
previous work (Finegan-Dollak et al., 2018)."
"In the cross-domain scenario, we randomly se-
lect M demonstration databases, each with K
NLQ-SQL pairs (M × K = N ) from the Spider
training set. Incorporating multiple demonstration
databases in a prompt text significantly increases
its length. Hence, we only use Codex for the cross-
domain experiments, due to its higher token limit
of 8K, surpassing the 4K limit of ChatGPT. In both
single-domain and cross-domain settings, we com-
pare different prompt construction methods using
the same few-shot examples to make a fair compar-
ison. We repeat our experiments three times and
present the average results.

5 Results

In this section, we present our empirical findings
in the areas of zero-shot, single-domain, and cross-
domain text-to-SQL. Through our experiments, we
aim to answer a few crucial research questions in
each setting and provide insightful strategies for
future studies on effective prompting.

5.1 Zero-shot Text-to-SQL

In the zero-shot setting, we focus on comparing
different prompt constructions for databases. Table

2We employ the Code-davinci-002 version of Codex across
all settings. In zero-shot and single-domain setups, we uti-
lize the gpt-3.5-turbo-0301 version of ChatGPT. For cross-
domain experiments involving ChatGPT-16K, we turned to
gpt-3.5-turbo-16k-0613 due to its extended context length.

Codex

ChatGPT

Database Prompt Construction

# Tokens (U|N)

EX (U|N)

# Tokens (U|N)

EX (U|N)

Table Schema

+Relationship

+Relationship+Content"
"Codex

ChatGPT

Database Prompt Construction

# Tokens (U|N)

EX (U|N)

# Tokens (U|N)

EX (U|N)

Table Schema

+Relationship

+Relationship+Content

Table(Columns)

Columns=[]

148 | 147

69.0 | 71.9

118 | 115

68.8 | 70.5

169 | 167

70.2 | 71.8

137 | 135

68.3 | 69.1

Columns=[]+ForeignKey

226 | 223

72.3 | 73.1

178 | 174

72.9 | 71.2

CreateTable

474 | 356

71.8 | 73.1

339 | 254

70.7 | 71.7

CreateTable+InsertRow 3

CreateTable+SelectRow 3

CreateTable+SelectCol 3

1089 | 1013

70.9 | 71.9

964 | 872

71.8 | 71.8

820 | 770

73.3 | 74.1

761 | 674

71.8 | 72.1

958 | 831

75.0 | 75.7

799 | 712

73.3 | 73.6

Table 1: Zero-shot results of Codex and ChatGPT using different database prompt constructions. Table Schema
(upper part) contains prompts that solely include the schema of tables, while +Relationship (middle part) incor-
porates foreign keys as the table relationships and +Relationship+Content (lower part) adds table content as
well. # Tokens is the average token counts in the prompts and EX represents the execution accuracy of SQLs. U|N
represents the results of unnormalized prompts and normalized prompts, respectively. The underlines highlight
the lower number of tokens and higher accuracies when comparing unnormalized and normalized prompts and the
highest accuracy achieved among all prompts is highlighted in bold."
"1 shows the average prompt length and execution
accuracy of Codex and ChatGPT using various
database prompt constructions.
Q1: How does normalized database prompt
perform compared to unnormalized ones? Nor-
malized schemas are found to have a reduced to-
ken count in comparison to unnormalized schemas
across all database constructions. The normaliza-
tion also tends to yield slightly better performance.
As for Codex, normalized schemas show improve-
ment in all prompts. For ChatGPT, normalized
schemas either improve accuracy or achieve the
same accuracy or achieve the same level of accu-
racy as unnormalized schemas in 6 out of 7 schema
constructions. The tests of statistical significance
are presented in Appendix A.2.
Q2: What database knowledge is crucial for
effectively prompting LLMs? Our experiments
indicate that table relationships and content are im-
portant. The Columns=[] prompt includes only the
table schema, while the Columns=[]+ForeignKey
prompt contains the additional relationship among
Including such
tables shown as foreign keys.
information improves the performance of both
Codex (71.8 -> 73.1) and ChatGPT (69.1 -> 71.2).
Moveover, exposing LLMs to database content
with the SelectRow and SelectCol prompts fur-
ther enhances the performance of both Codex and
ChatGPT, while the InsertRow prompt does not
seem to be beneficial. We believe that database
content is valuable, but its representation needs to
be carefully chosen.
Q3: How does Codex perform compared to"
"ChatGPT? While we do not focus on comparing
different LLMs on the text-to-SQL tasks in this
paper, it is worth noting that Codex consistently
outperforms ChatGPT on zero-shot text-to-SQL
using various prompt constructions.

Based on all the findings above, we would recom-
mend using Codex in conjunction with normalized
CreateTableSelectCol prompt construction for
zero-shot text-to-SQL.3

5.2 Single-domain Text-to-SQL

In the zero-shot text-to-SQL setting, we discovered
that the prompt constructions of databases impact
the performance of LLMs. This discovery naturally
raises the question of whether the introduction of
in-domain demonstrations affects the performance
of LLMs to different database prompts.
Q1: Does the use of in-domain demonstrations
enhance LLM’s performance? Figure 5 depicts
the performance of Codex and ChatGPT using dif-
ferent database prompt constructions with respect
to different numbers of in-domain demonstration
examples. For all database prompts, the perfor-
mance of LLMs experiences a notable improve-
ment when in-domain examples are presented. Fur-
thermore, the performance continues to enhance as
the number of in-domain examples increases.
Q2: What database knowledge is important
when presenting in-domain demonstrations?
While we have observed that the presence of table

3To simplify our experiments and ensure consistent
prompts, we adopt normalization for single-domain and cross-
domain experiments."
"3To simplify our experiments and ensure consistent
prompts, we adopt normalization for single-domain and cross-
domain experiments.

scenario, CreateTable+SelectCol outperforms
CreateTable by 2.0% for Codex and 1.7% for
ChatGPT. As we proceed to increase the number
of in-domain examples, we observe that the per-
formance gap between these two prompts does not
exhibit a significant reduction. Even with 16 in-
domain examples, the gap still persists at 1.3 for
Codex and 1.9 for ChatGPT."
"These results indicate LLMs are able to quickly
learn table relationships from a small number of in-
domain demonstrations, however, it is more chal-
lenging to obtain table content knowledge from
demonstration examples. Consequently, the inclu-
sion of table content remains crucial for achieving
satisfactory performance in the single-domain text-
to-SQL scenario.
Q3: Can in-domain demonstrations alleviate the
sensitivity of LLMs to the representation of ta-
ble content? In the zero-shot setting, we observe
that LLMs are sensitive to how the table content is
presented. Specifically, SelectCol 3 outperforms
InsertRow 3 by a substantial margin of 3.8 for
Codex and 1.8 for ChatGPT. However, as we ex-
pose LLMs to in-domain demonstrations, LLMs be-
come less sensitive to the specific representation of
table content. The performance disparities among
the three table content prompts become marginal.
Notably, with only 4 examples, the performance
difference between SelectCol 3 and InsertRow
3 diminishes to 0.3 for Codex and 0.2 for ChatGPT.
To summarize, in single-domain text-to-SQL,
we recommend incorporating a greater number of
in-domain examples whenever feasible. It is also
essential to ensure the presence of table content
in conjunction with the table schema while the
specific choice of table content construction is less
crucial compared to the zero-shot scenario.

5.3 Cross-domain Text-to-SQL"
"5.3 Cross-domain Text-to-SQL

In this section, we present the results to answer a
series of questions regarding the demonstration and
database prompt construction.

5.3.1

Impact of Demonstration Prompt

To investigate the impact of the number of
databases and examples per database in demon-
strations, we conduct experiments encompassing
various combinations. Specifically, our demonstra-
tions are composed of M demonstration databases,
each containing K NLQ-SQL pairs. We consider
scenarios with up to 8 databases and 16 examples
per database as long as the combination does not

(a) Codex

(b) ChatGPT

Figure 5: Execution accuracy of Codex and ChatGPT
for single-domain text-to-SQL with 1, 4, 8, and 16 in-
domain examples. RS and Cont correspond to table
relationship and table content, respectively. Detailed
results can be found in Table 4 and 5.

relationships and table content enhanced LLMs’
performance in the zero-shot scenario, it is not
clear whether they are still important in the single-
domain setting. A hypothesis is that table relation-
ship and table content knowledge can be acquired
from in-domain examples as they may appear in
SQL clauses JOIN and WHERE.

For"
"For

table relationships, we compare two
database prompt constructions Columns=[] and
Columns=[]+ForeignKey. Both construct the ta-
ble schema in the same way while the latter in-
cludes foreign keys as table relationships. In the
zero-shot scenario, Columns=[]+ForeignKey out-
performs Columns=[] by 1.3 and 2.1 for Codex
and ChatGPT, respectively. However, as increasing
the number of in-domain examples, we notice a
gradual reduction in the performance gap between
these two prompts. With the utilization of 16 in-
domain examples, the gap completely disappears
for Codex, while ChatGPT exhibits a marginal dif-
ference of only 0.5%.

For table content, we compare CreateTable
with CreateTable+SelectCol. Both contain
the same prompts for presenting the table
schema and relationship, while the latter addi-
tionally includes table content. In the zero-shot"
"For table content, we compare CreateTable
with CreateTable+SelectCol. Both contain
the same prompts for presenting the table
schema and relationship, while the latter addi-
tionally includes table content. In the zero-shot

05101570727476788082TableTable(Columns)Columns=[]Table+RSColumns=[]+ForeignKeyCreateTableTable+RS+ContCreateTable+InsertRowCreateTable+SelectRowCreateTable+SelectCol# ShotsExecution Accuracy051015707274767880TableTable(Columns)Columns=[]Table+RSColumns=[]+ForeignKeyCreateTableTable+RS+ContCreateTable+InsertRowCreateTable+SelectRowCreateTable+SelectCol# ShotsExecution Accuracyor 16 examples per database leads to a significant
decrease in the Codex’s performance compared to
using 2 or 4 examples per database. Secondly, for
a fixed number of examples per database, we ob-
serve an initial increase in Codex’s performance as
the number of databases increases, however, this
improvement is followed by a significant decrease
once the number of databases reaches a certain
threshold (either 4 or 6).
Q2: Why does increasing the number of
databases decrease LLMs’ performance? As
depicted in Figure 2, presenting more databases
does not always lead to improved performance. In
fact, there is a significant decline in performance,
once it surpasses a threshold. We hypothesize that
this phenomenon is attributed to the length of the
prompt text. To test this hypothesis, we analyze the
results in relation to the prompt length."
"Figure 7 shows the relationship between the
accuracy of different demonstration prompts and
their prompt lengths. Notably, the performance of
Codex exhibits an inverted-U shape as the prompt
length increases for each number of examples per
database. Additionally, we observe a substantial
drop in performance once the prompt text length
exceeds approximately 5500 tokens. Similarly, Fig-
ure 9 shows that the performance of ChatGPT-16K
starts to decrease when prompt text length exceeds
11K tokens. Based on these observations, we con-
jecture that LLMs may have a sweet spot in terms
of prompt length, potentially influenced by factors
such as their model architecture or training data.
This indicates that even though LLMs are capable
of handling long contexts, they may not necessarily
perform better with excessively long prompts.

5.3.2

Impact of Database Prompt"
"5.3.2

Impact of Database Prompt

Since incorporating demonstration databases may
cause a decrease in Codex’s performance, we fo-
cus our database prompt experiments on using one
demonstration database in combination with vary-
ing quantities of demonstration examples. Table
2 presents the execution accuracy of Codex using
different database prompts.
Q3: Do different database prompts show similar
trends with the number of demonstration exam-
ples? We observe an initial performance increase
for all database prompts. However, once more than
4 examples are provided, the improvement starts
to level off, indicating that the different database
prompts exhibit similar trends in relation to the
number of demonstration examples.

Figure 6: A heat map of Codex’s execution accuracy us-
ing CreateTable+SelectRow 3 for different numbers
of databases and examples per database in the demon-
stration. Darker color indicates higher accuracy.

Figure 7: Execution accuracy of Codex in relation to
the length of prompts. Each dot on the graph represents
a specific demonstration prompt construction, with the
m, k denoting the number of databases and examples
per database used in the prompt. The lines represent
second-degree polynomial trendlines fitted to the results."
"exceed the prompt length limit. We opt to use the
database prompt CreateTable+SelectRow 3 as
it contains fewer tokens compared to InsertRow
and SelectCol while encompassing all valuable
database knowledge. We present the experiments
with Codex in this section. Experiments involv-
ing ChatGPT-16K can be found in Appendix A.4
which show similar results as Codex.
Q1: Does increasing demonstration examples
enhance LLMs’ performance? Figure 2 presents
the accuracy of Codex corresponding to different
combinations of the number of databases and the
number of examples per database used as demon-
strations. We analyze the results from two perspec-
tives. Firstly, for a fixed number of databases, we
observe an initial improvement in Codex’s perfor-
mance as the number of examples per database
increases. However, this improvement plateaus or
declines once 4 examples per database are provided.
Surprisingly, when using 4 databases, employing 8

12468# DBs168421# Examples/DB76.076.074.575.775.974.276.275.776.375.075.075.276.174.773.873.073.373.973.372.572.573.073.574.074.575.075.576.01,12,14,16,18,11,22,24,26,28,21,42,44,46,41,82,84,81,162,164,1620003000400050006000700072737475761 example/DB2 examples/DB4 examples/DB8 examples/DB16 examples/DB# Prompt TokensExecution AccuracyDatabase Prompt Construction

0-shot

1-shot

2-shot

4-shot

8-shot

16-shot

Table Schema

+Relationship

+Relationship+Content

Table(Columns)

Columns=[]

Columns=[]+ForeignKey

CreateTable"
"0-shot

1-shot

2-shot

4-shot

8-shot

16-shot

Table Schema

+Relationship

+Relationship+Content

Table(Columns)

Columns=[]

Columns=[]+ForeignKey

CreateTable

CreateTable+InsertRow 3

CreateTable+SelectRow 3

CreateTable+SelectCol 3

71.9

71.8

73.1

73.1

71.9

74.1

75.7

72.0

71.9

73.3

72.1

72.2

73.0

74.4

73.0

73.6

74.5

73.4

74.1

75.0

75.5

73.2

74.2

74.9

73.7

74.9

76.2

76.5

72.8

73.7

74.9

74.1

74.9

75.7

76.8

73.9

74.4

75.2

75.1

74.8

76.0

76.5

Table 2: Cross-domain results of Codex using different database prompt constructions. Only one demonstration
database is included in a prompt, N-shot represents N examples corresponding to the demonstration database. The
best and second-best results for each shot are highlighted in bold and underlined."
"Q4: Can out-of-domain demonstrations allevi-
ate the sensitivity of LLMs to database prompts?
First, we observe that incorporating table relation-
ships and content in the prompts remains crucial for
effectively prompting Codex in the cross-domain
setting. This is not surprising, as Codex cannot di-
rectly learn knowledge specific to the test database
from the out-of-domain demonstrations. Further-
more, we find that Codex continues to exhibit sensi-
tivity to the representation of table content. Despite
having demonstration databases that mirror the con-
struction of the test database, Codex still displays
a preference forSelectRow and SelectCol when
presenting table content, compared to InsertCol.
In conclusion, while out-of-domain demonstra-
tions enhance LLMs’ capabilities in text-to-SQL,
they do not provide database-specific knowledge.
Consequently, careful construction of database
prompts remains crucial, aligning with the observa-
tions made in the zero-shot setting.

6 Related Work"
"6 Related Work

LLMs for Text-to-SQL In recent years, there
has been significant progress in leveraging LLMs
for the text-to-SQL task. Various methods have
been proposed to enhance the capabilities of LLMs.
For example, Rubin et al. (2021); Poesia et al.
(2022) have demonstrated the effectiveness of
similarity-based demonstration retrieval in the
cross-domain setting. Additionally, Levy et al.
(2022) have highlighted the advantages of incor-
porating diverse demonstrations for compositional
generalization. Furthermore, Pourreza and Rafiei
(2023) and Chen et al. (2023) incorporate interme-
diate steps in prompts and unlock LLMs’ capability
of self-correcting their predictions.

In contrast to these approaches, our focus lies in

conducting a comprehensive evaluation of prompt
representations across different text-to-SQL set-
tings. While there are similar motivations to the
work by Rajkumar et al. (2022), which analyzes the
performance of CodeX on Spider for the zero-shot
setting and on two databases for the single-domain
setting, we aim to provide more general findings by
evaluating across a wider range of databases and
considering all three text-to-SQL settings."
"Table Representation Encoding structured
databases with neural models has been a persistent
challenge. To encode database schema, graph
neural networks are utilized to represent the rela-
tionships among tables (Bogin et al., 2019; Chen
et al., 2021b). Alternatively, other studies (Guo
et al., 2019; Lin et al., 2020; Shaw et al., 2020)
have converted table schemas into a sequence to
effectively leverage pretrained language models,
such as BERT (Devlin et al., 2018) and T5 (Raffel
et al., 2020). In such cases, table relationships can
be encoded as meta-data features (Lin et al., 2020)
or used as a guide for attention mechanism (Wang
et al., 2019; Cao et al., 2021; Li et al., 2023b).

To incorporate table content into neural mod-
els, prior supervised methods provide question-
specific table content by identifying the relevant
table content mentioned in the question through
string matching (Lin et al., 2020; Shaw et al., 2020).
However, Chang et al. (2023) have revealed the
vulnerability of string matching to perturbations.
Given that LLMs with in-context learning support
longer input sequences compared to supervised
methods, we follow previous work to provide table
content without explicitly considering the questions
(Rajkumar et al., 2022; Chen et al., 2023).

7 Conclusions

References"
"7 Conclusions

References

In this paper, we investigate effective prompt-
ing strategies in the text-to-SQL task. We thor-
oughly compare various prompt construction strate-
gies for databases and demonstrations in the zero-
shot, single-domain, and cross-domain text-to-SQL.
Through our investigation, we uncover the critical
database knowledge and optimal representations
for effective prompting. Additionally, an interest-
ing finding is the existence of a sweet spot in terms
of prompt length for Codex in the cross-domain
setting. Overall, we believe that our findings will
provide valuable guidance for future research in
the field of text-to-SQL with LLMs.

Limitation

We conducted our experiments using 20 databases
from the Spider dataset, with the goal of providing
general findings for text-to-SQL prompt construc-
tions. However, our findings may not always be
applicable to a specific database, particularly if the
database is significantly different from the Spider
databases. For the single-domain and cross-domain
text-to-SQL scenarios, we conduct our experiments
multiple times, each involving randomly selecting
demonstrations with different random seeds, how-
ever, we did not investigate the effectiveness of
prompt constructions with different demonstration-
retrieval strategies or intermediate reasoning steps.

Ethics Statement"
"Ethics Statement

We acknowledge the importance of the ACL Ethics
Policy and agree with it. In this paper, we use Ope-
nAI Codex and ChatGPT as our language models
4. Codex is currently free for research purposes,
the cost of ChatGPT is around $200. The code
for the paper is included in the supplementary ma-
terials and will be publicly released to facilitate
reproducibility.

4API is available at https://openai.com/api/.

Ben Bogin, Matt Gardner, and Jonathan Berant. 2019.
Representing schema structure with graph neural
arXiv preprint
networks for text-to-sql parsing.
arXiv:1905.06241.

Tom Brown, Benjamin Mann, Nick Ryder, Melanie
Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind
Neelakantan, Pranav Shyam, Girish Sastry, Amanda
Askell, et al. 2020. Language models are few-shot
learners. Advances in neural information processing
systems, 33:1877–1901.

Ruisheng Cao, Lu Chen, Zhi Chen, Yanbin Zhao,
Su Zhu, and Kai Yu. 2021. Lgesql: line graph en-
hanced text-to-sql model with mixed local and non-
local relations. arXiv preprint arXiv:2106.01093.

Shuaichen Chang, Jun Wang, Mingwen Dong, Lin Pan,
Henghui Zhu, Alexander Hanbo Li, Wuwei Lan,
Sheng Zhang, Jiarong Jiang, Joseph Lilien, et al.
2023. Dr. spider: A diagnostic evaluation bench-
mark towards text-to-sql robustness. arXiv preprint
arXiv:2301.08881."
"Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan,
Henrique Ponde de Oliveira Pinto, Jared Kaplan,
Harri Edwards, Yuri Burda, Nicholas Joseph, Greg
Evaluating large lan-
Brockman, et al. 2021a.
arXiv preprint
guage models trained on code.
arXiv:2107.03374.

Xinyun Chen, Maxwell Lin, Nathanael Schärli, and
Denny Zhou. 2023. Teaching large language models
to self-debug. arXiv preprint arXiv:2304.05128.

Zhi Chen, Lu Chen, Yanbin Zhao, Ruisheng Cao, Zi-
han Xu, Su Zhu, and Kai Yu. 2021b. Shadowgnn:
Graph projection neural network for text-to-sql parser.
arXiv preprint arXiv:2104.04689.

Zhoujun Cheng, Tianbao Xie, Peng Shi, Chengzu
Li, Rahul Nadkarni, Yushi Hu, Caiming Xiong,
Dragomir Radev, Mari Ostendorf, Luke Zettlemoyer,
et al. 2022. Binding language models in symbolic
languages. arXiv preprint arXiv:2210.02875.

Aakanksha Chowdhery, Sharan Narang, Jacob Devlin,
Maarten Bosma, Gaurav Mishra, Adam Roberts,
Paul Barham, Hyung Won Chung, Charles Sutton,
Sebastian Gehrmann, et al. 2022. Palm: Scaling
language modeling with pathways. arXiv preprint
arXiv:2204.02311.

Deborah A. Dahl, Madeleine Bates, Michael Brown,
William Fisher, Kate Hunicke-Smith, David Pallett,
Christine Pao, Alexander Rudnicky, and Elizabeth
Shriberg. 1994. Expanding the scope of the atis task:
The atis-3 corpus. In Proceedings of the Workshop
on Human Language Technology, pages 43–48."
"Jacob Devlin, Ming-Wei Chang, Kenton Lee, and
Kristina Toutanova. 2018. Bert: Pre-training of deep
bidirectional transformers for language understand-
ing. arXiv preprint arXiv:1810.04805.

Nitarshan Rajkumar, Raymond Li, and Dzmitry Bah-
danau. 2022. Evaluating the text-to-sql capabil-
arXiv preprint
ities of large language models.
arXiv:2204.00498.

Ohad Rubin and Jonathan Berant. 2021. Smbop: Semi-
autoregressive bottom-up semantic parsing. In Pro-
ceedings of the 2021 Conference of the North Amer-
ican Chapter of the Association for Computational
Linguistics: Human Language Technologies, pages
311–324.

Ohad Rubin, Jonathan Herzig, and Jonathan Berant.
2021. Learning to retrieve prompts for in-context
learning. arXiv preprint arXiv:2112.08633.

Torsten Scholak, Nathan Schucher, and Dzmitry Bah-
danau. 2021. Picard: Parsing incrementally for
constrained auto-regressive decoding from language
models. In Proceedings of the 2021 Conference on
Empirical Methods in Natural Language Processing,
pages 9895–9901.

Peter Shaw, Ming-Wei Chang, Panupong Pasupat, and
Kristina Toutanova. 2020. Compositional general-
ization and natural language variation: Can a seman-
tic parsing approach handle both? arXiv preprint
arXiv:2010.12725.

Peng Shi, Rui Zhang, He Bai, and Jimmy Lin. 2022.
Xricl: Cross-lingual retrieval-augmented in-context
learning for cross-lingual text-to-sql semantic pars-
ing. arXiv preprint arXiv:2210.13693."
"Peng Shi, Rui Zhang, He Bai, and Jimmy Lin. 2022.
Xricl: Cross-lingual retrieval-augmented in-context
learning for cross-lingual text-to-sql semantic pars-
ing. arXiv preprint arXiv:2210.13693.

Bailin Wang, Richard Shin, Xiaodong Liu, Oleksandr
Polozov, and Matthew Richardson. 2019. Rat-sql:
Relation-aware schema encoding and linking for text-
to-sql parsers. arXiv preprint arXiv:1911.04942.

Tao Yu, Chien-Sheng Wu, Xi Victoria Lin, Bailin
Wang, Yi Chern Tan, Xinyi Yang, Dragomir R Radev,
Richard Socher, and Caiming Xiong. 2021. Grappa:
Grammar-augmented pre-training for table semantic
parsing. In ICLR.

Tao Yu, Rui Zhang, Kai Yang, Michihiro Yasunaga,
Dongxu Wang, Zifan Li, James Ma, Irene Li, Qingn-
ing Yao, Shanelle Roman, et al. 2018. Spider: A
large-scale human-labeled dataset for complex and
cross-domain semantic parsing and text-to-sql task.
arXiv preprint arXiv:1809.08887.

John M. Zelle and Raymond J. Mooney. 1996. Learn-
ing to parse database queries using inductive logic
programming. In Proceedings of the Thirteenth Na-
tional Conference on Artificial Intelligence - Volume
2, pages 1050–1055.

Catherine Finegan-Dollak, Jonathan K Kummerfeld,
Li Zhang, Karthik Ramanathan, Sesh Sadasivam,
Rui Zhang, and Dragomir Radev. 2018. Improving
text-to-sql evaluation methodology. arXiv preprint
arXiv:1806.09029."
"Catherine Finegan-Dollak, Jonathan K Kummerfeld,
Li Zhang, Karthik Ramanathan, Sesh Sadasivam,
Rui Zhang, and Dragomir Radev. 2018. Improving
text-to-sql evaluation methodology. arXiv preprint
arXiv:1806.09029.

Yujian Gan, Xinyun Chen, Jinxia Xie, Matthew Purver,
John R Woodward, John Drake, and Qiaofu Zhang.
2021. Natural sql: Making sql easier to infer
from natural language specifications. arXiv preprint
arXiv:2109.05153.

Jiaqi Guo, Zecheng Zhan, Yan Gao, Yan Xiao, Jian-
Guang Lou, Ting Liu, and Dongmei Zhang. 2019. To-
wards complex text-to-sql in cross-domain database
with intermediate representation. arXiv preprint
arXiv:1905.08205.

Itay Levy, Ben Bogin, and Jonathan Berant. 2022.
in-context
arXiv preprint

Diverse
compositional generalization.
arXiv:2212.06800.

demonstrations

improve

Haoyang Li, Jing Zhang, Cuiping Li, and Hong
Chen. 2023a. Decoupling the skeleton parsing
and schema linking for text-to-sql. arXiv preprint
arXiv:2302.05965.

Jinyang Li, Binyuan Hui, Reynold Cheng, Bowen Qin,
Chenhao Ma, Nan Huo, Fei Huang, Wenyu Du, Luo
Si, and Yongbin Li. 2023b. Graphix-t5: Mixing pre-
trained transformers with graph-aware layers for text-
to-sql parsing. arXiv preprint arXiv:2301.07507.

Xi Victoria Lin, Richard Socher, and Caiming Xiong.
2020. Bridging textual and tabular data for cross-
In Findings
domain text-to-sql semantic parsing.
of the Association for Computational Linguistics:
EMNLP 2020, pages 4870–4888."
"Xi Victoria Lin, Richard Socher, and Caiming Xiong.
2020. Bridging textual and tabular data for cross-
In Findings
domain text-to-sql semantic parsing.
of the Association for Computational Linguistics:
EMNLP 2020, pages 4870–4888.

Aiwei Liu, Xuming Hu, Lijie Wen, and Philip S
Yu. 2023. A comprehensive evaluation of chat-
gpt’s zero-shot text-to-sql capability. arXiv preprint
arXiv:2303.13547.

Gabriel Poesia, Oleksandr Polozov, Vu Le, Ashish Ti-
wari, Gustavo Soares, Christopher Meek, and Sumit
Gulwani. 2022. Synchromesh: Reliable code gen-
eration from pre-trained language models. arXiv
preprint arXiv:2201.11227.

Mohammadreza Pourreza and Davood Rafiei. 2023.
learning of
arXiv preprint

Din-sql: Decomposed in-context
text-to-sql with self-correction.
arXiv:2304.11015.

Patti Price. 1990. Evaluation of spoken language sys-
tems: The atis domain. In Speech and Natural Lan-
guage: Proceedings of a Workshop Held at Hidden
Valley, Pennsylvania, June 24-27, 1990.

Colin Raffel, Noam Shazeer, Adam Roberts, Katherine
Lee, Sharan Narang, Michael Matena, Yanqi Zhou,
Wei Li, Peter J Liu, et al. 2020. Exploring the limits
of transfer learning with a unified text-to-text trans-
former. J. Mach. Learn. Res., 21(140):1–67.

A Appendix

A.1 Prompt Examples"
"A Appendix

A.1 Prompt Examples

Below contains an example of a zero-shot nor-
malized prompt, which contains the database
Network_1 from Spider (Yu et al., 2018), a task
instruction “Using valid SQLite, answer the fol-
lowing questions for the tables provided above.”,
and a test question “How many high schoolers are
there?”.

Zero-shot normalized prompt

create table highschooler (
id int primary key ,
name text ,
grade int
);
/*
3 example rows :
select * from highschooler limit 3;
id
1510
1689
1381
*/

Jordan
Gabriel
Tiffany

grade
9

name

9
9

create table friend (
student_id int ,
friend_id int ,
primary key ( student_id , friend_id ) ,
foreign key ( student_id ) references
highschooler ( id ) ,
foreign key ( friend_id ) references
highschooler ( id )
);
/*
3 example rows :
select * from friend limit 3;
student_id
1510
1510
1689
*/

1381
1689
1709

friend_id

create table likes (
student_id int ,
liked_id int ,
primary key ( student_id , liked_id ) ,
foreign key ( liked_id ) references
highschooler ( id ) ,
foreign key ( student_id ) references
highschooler ( id )
);
/*
3 example rows :
select * from likes limit 3;
student_id
1689
1709
1782
*/

1709
1689
1709

liked_id

-- Using valid SQLite , answer the following
questions for the tables provided above .

Question : How many high schoolers are there ?
select"
"1709
1689
1709

liked_id

-- Using valid SQLite , answer the following
questions for the tables provided above .

Question : How many high schoolers are there ?
select

Below contains an example of a 4-shot single-
domain normalized prompt, which contains a
database prompt and 4 demonstration examples
ahead of the test question.

4-shot single-domain normalized prompt

create table highschooler (
id int primary key ,
name text ,
grade int
);
/*
3 example rows :
select * from highschooler limit 3;
id
1510
1689
1381
*/

Jordan
Gabriel
Tiffany

grade
9

name

9
9

create table friend (
student_id int ,
friend_id int ,
primary key ( student_id , friend_id ) ,
foreign key ( student_id ) references
highschooler ( id ) ,
foreign key ( friend_id ) references
highschooler ( id )
);
/*
3 example rows :
select * from friend limit 3;
student_id
1510
1510
1689
*/

1381
1689
1709

friend_id

create table likes (
student_id int ,
liked_id int ,
primary key ( student_id , liked_id ) ,
foreign key ( liked_id ) references
highschooler ( id ) ,
foreign key ( student_id ) references
highschooler ( id )
);
/*
3 example rows :
select * from likes limit 3;
student_id
1689
1709
1782
*/

1709
1689
1709

liked_id

-- Using valid SQLite , answer the following
questions for the tables provided above .

Question : What is Kyle 's id ?
select id from highschooler where name = '
Kyle ';
Question : Return the names of friends of the

high school student Kyle ."
"-- Using valid SQLite , answer the following
questions for the tables provided above .

Question : What is Kyle 's id ?
select id from highschooler where name = '
Kyle ';
Question : Return the names of friends of the

high school student Kyle .

select t3 . name from friend as t1 join
highschooler as t2 on t1 . student_id = t2 . id
join highschooler as t3 on t1 . friend_id = t3
. id where t2 . name = ' Kyle ';
Question : Show names of all high school
students who do not have any friends .
select name from highschooler except select
t2 . name from friend as t1 join highschooler
as t2 on t1 . student_id = t2 . id ;
Question : What are the names and grades for
each high schooler ?
select name , grade from highschooler ;
Question : How many high schoolers are there ?
select

Below contains an example of a 4-shot cross-
domain prompt, which contains 2 demonstration
databases, each with 2 demonstration examples
ahead of the test database and question.

4-shot cross-domain prompt

create table publication (
publication_id int ,
book_id int ,
publisher text ,
publication_date text ,
price real ,
primary key ( publication_id ) ,
foreign key ( book_id ) references book (
book_id )
);
/*
3 example rows :
select * from publication limit 3;
publication_id
publication_date
1
1
15000000.0
2
3
6000000.0
3
*/

Thomson Reuters

book_id
price

August 2008

June 2006

publisher

Pearson

Wiley

4

March 2008

4100000.0"
"Thomson Reuters

book_id
price

August 2008

June 2006

publisher

Pearson

Wiley

4

March 2008

4100000.0

create table book (
book_id int ,
title text ,
issues real ,
writer text ,
primary key ( book_id )
);
/*
3 example rows :
select * from book limit 3;
book_id
title
1
2
3
Garth Ennis
*/

The Black Lamb
Bloody Mary
Bloody Mary : Lady Liberty

issues
6.0

4.0

writer
Timothy Truman

Garth Ennis
4.0

-- Using valid SQLite , answer the following
questions for the tables provided above .

Question : List the writers of the books in
ascending alphabetical order .
select writer from book order by writer asc ;
Question : How many books are there ?
select count (*) from book ;

create table race (
race_id int ,
name text ,
class text ,
date text ,
track_id text ,
primary key ( race_id ) ,
foreign key ( track_id ) references track (
track_id )
);
/*
3 example rows :
select * from race limit 3;
race_id
name
1

class
Rolex 24 At Daytona

date
DP / GT

track_id
January

26 January 27

1
Gainsco Grand Prix of Miami

DP / GT

March 29

2

Mexico City 250

DP / GT

April 19

2

3

2

*/

create table track (
track_id int ,
name text ,
location text ,

seating real ,
year_opened real ,
primary key ( track_id )
);
/*
3 example rows :
select * from track limit 3;
location
name
track_id
year_opened
1
92000.0
2
75000.0
3

2001.0
Darlington Raceway

Auto Club Speedway
1997.0

Chicagoland Speedway

seating

Fontana , CA

Joliet , IL

Darlington , SC

63000.0

1950.0

*/"
"2001.0
Darlington Raceway

Auto Club Speedway
1997.0

Chicagoland Speedway

seating

Fontana , CA

Joliet , IL

Darlington , SC

63000.0

1950.0

*/

-- Using valid SQLite , answer the following
questions for the tables provided above .
Question : Show the name and location for all

tracks .

select name , location from the track ;
Question : Show the name of track and the
number of races in each track .
select t2 . name , count (*) from race as t1
join track as t2 on t1 . track_id = t2 .
track_id group by t1 . track_id ;

create table highschooler (
id int primary key ,
name text ,
grade int
);
/*
3 example rows :
select * from highschooler limit 3;
id
1510
1689
1381
*/

Jordan
Gabriel
Tiffany

grade
9

name

9
9

create table friend (
student_id int ,
friend_id int ,
primary key ( student_id , friend_id ) ,
foreign key ( student_id ) references
highschooler ( id ) ,
foreign key ( friend_id ) references
highschooler ( id )
);
/*
3 example rows :
select * from friend limit 3;
student_id
1510
1510
1689
*/

1381
1689
1709

friend_id

create table likes (
student_id int ,
liked_id int ,
primary key ( student_id , liked_id ) ,
foreign key ( liked_id ) references
highschooler ( id ) ,
foreign key ( student_id ) references
highschooler ( id )
);
/*
3 example rows :
select * from likes limit 3;
student_id
1689
1709
1782
*/

1709
1689
1709

liked_id

-- Using valid SQLite , answer the following
questions for the tables provided above ."
"1709
1689
1709

liked_id

-- Using valid SQLite , answer the following
questions for the tables provided above .

Question : How many high schoolers are there ?
select

A.2 Tests of Significance

Table 1 contains the performance of Codex and
ChatGPT using different database prompt construc-
tions in the zero-shot setting. We observe that the
normalization results in slightly improved perfor-
mance for all database prompt constructions with
Codex and 6 out of 7 database prompt construc-
tions with ChatGPT. It is important to note, how-
ever, that when comparing normalized and unnor-
malized database prompt constructions using the
same method, the results did not demonstrate statis-
tical significance in McNemar’s test, with p-values
greater than 0.05. Nevertheless, the primary advan-
tage of normalization lies in its ability to reduce
variations among different databases and minimize
the overall prompt length.

When

prompt

various

evaluating"
"When

prompt

various

evaluating

con-
structions, we note the advantages gained
from incorporating both table relationships
(Columns=[]+ForeignKey vs Columns=[]) and
table content (CreateTable+SelectCol
3 vs
CreateTable) are mostly statistically significant
in McNemar’s test, with p-values smaller than 0.05.
Table 3 displays the results of the significant tests.
The performance of Columns=[]+ForeignKey
compared to Columns=[] is statistically sig-
nificant
in all cases, except for codex with
normalized prompts. Likewise, the performance
of CreateTable+SelectCol
3 is statistically
significant for both Codex and ChatGPT, with both
normalized and unnormalized prompts, when com-
pared to CreateTable. These significant findings
highlight the effectiveness of incorporating table
relationships and database content.

A.3 Detailed Single-domain Results

Tables 4 and 5 provide detailed results of Codex
and ChatGPT in the single-domain setting, respec-
tively. The performance of both models is also
illustrated in Figure 5.

A.4

Impact of Demonstration Prompt for
ChatGPT-16K

Figure 8 presents the accuracy of ChatGPT-16K
corresponding to different combinations of the
number of databases and the number of examples
per database used as demonstrations. Similar to our
findings with Codex, presenting more databases
does not always lead to improved performance for
ChatGPT-16K. For a fixed number of examples
per database, we observe an initial increase in its"
"Figure 8: A heat map of ChatGPT-16K’s execution ac-
curacy using CreateTable+SelectRow 3 for different
numbers of databases and examples per database in the
demonstration. Darker color indicates higher accuracy.

Figure 9: Execution accuracy of ChatGPT-16K in re-
lation to the length of prompts. Each dot represents a
demonstration construction, with the m, k denoting the
number of databases and examples per database. The
lines represent second-degree polynomial trendlines fit-
ted to the results.

performance as the number of databases increases,
however, this improvement is followed by a de-
crease once the number of databases reaches a cer-
tain threshold. To understand this phenomenon, we
analyze the results in relation to the prompt length.
Figure 9 shows the relationship between the ac-
curacy of different demonstration prompts and their
prompt lengths. Similar to Codex, the performance
of ChatGPT-16K also exhibits an inverted-U shape
as the prompt length increases for each number of
examples per database. Additionally, we observe
the performance starts to decrease once the prompt
text length exceeds approximately 11K tokens.

While Codex supports 8K tokens and ChatGPT-
16K supports 16K tokens, we notice that their
performance tends to decline when dealing with
demonstrations that exceed approximately 70% of
the maximum prompt length."
"While Codex supports 8K tokens and ChatGPT-
16K supports 16K tokens, we notice that their
performance tends to decline when dealing with
demonstrations that exceed approximately 70% of
the maximum prompt length.

12481216# DBs168421# Examples/DB77.077.077.676.475.876.477.477.975.974.876.275.977.577.675.976.176.376.776.576.675.976.475.577.376.477.376.375.075.576.076.577.077.51,12,14,18,112,116,11,22,24,28,212,216,21,42,44,48,412,416,1,82,84,88,812,81,162,164,168,162k4k6k8k10k12k14k727374757677781 example/DB2 examples/DB4 examples/DB8 examples/DB16 examples/DB# Prompt TokensExecution AccuracyPrompt 1

Prompt 2

LLM

Normalization

Significant Test

Columns=[]

Columns=[]+ForeignKey

CreateTable

CreateTable+SelectCol 3

Codex

Codex

ChatGPT

ChatGPT

Codex

Codex

ChatGPT

ChatGPT

U

N

U

N

U

N

U

N

✓

✗

✓

✓

✓

✓

✓

✓

Table 3: Tests of Statistical Significance for comparing different prompt constructions. Prompt 1 and Prompt 2 were
used to represent two distinct methods of constructing prompts in McNemar’s test. The prompts were categorized
as U and N, representing unnormalized and normalized database prompts, respectively. The ✓ symbol indicates that
the p-value is smaller than 0.05, indicating statistical significance, while the ✗ symbol indicates p-values greater
than 0.05, indicating a lack of statistical significance.

Database Prompt Construction

0-shot

1-shot

4-shot

8-shot

16-shot

Table Schema

+Relationship

+Relationship+Content

Table(Columns)"
"Database Prompt Construction

0-shot

1-shot

4-shot

8-shot

16-shot

Table Schema

+Relationship

+Relationship+Content

Table(Columns)

Columns=[]

Columns=[]+ForeignKey

CreateTable

CreateTable+InsertRow 3

CreateTable+SelectRow 3

CreateTable+SelectCol 3

71.9

71.8

73.1

73.1

71.9

74.1

75.7

70.7

72.1

73.2

72.4

72.9

73.4

74.1

74.9

75.5

76.1

76.0

77.6

77.3

77.9

78.0

78.0

78.5

78.7

80.5

80.5

80.7

81.6

81.6

81.6

81.3

82.5

82.9

82.5

Table 4: Single-domain results of Codex using different prompt constructions for database schema and content. The
best and second-best results for each shot are highlighted in bold and underlined.

Database Prompt Construction

0-shot

1-shot

4-shot

8-shot

16-shot

Table Schema

+Relationship

+Relationship+Content

Table(Columns)

Columns=[]

Columns=[]+ForeignKey

CreateTable

CreateTable+InsertRow 3

CreateTable+SelectRow 3

CreateTable+SelectCol 3

70.5

69.1

71.2

71.7

71.8

72.1

73.6

71.6

70.7

73.4

73.1

72.8

73.3

73.8

74.3

74.4

75.4

75.8

76.6

76.4

76.8

77.4

77.8

78.4

78.0

79.1

78.9

79.8

79.4

79.5

80.0

79.5

81.6

81.3

81.4

Table 5: Single-domain results of ChatGPT using different prompt constructions for database schema and content.
The best and second-best results for each shot are highlighted in bold and underlined."
