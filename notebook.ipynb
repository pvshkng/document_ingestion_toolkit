{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bf7d3d3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bc52ccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dbdc90e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert PDF to MD\n",
    "\n",
    "import os\n",
    "from markitdown import MarkItDown\n",
    "\n",
    "input_dir = \"./files/pdf\"\n",
    "output_dir = \"./files/md\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "md = MarkItDown(enable_plugins=False)\n",
    "\n",
    "for filename in os.listdir(input_dir):\n",
    "    if filename.lower().endswith(\".pdf\"):\n",
    "        input_path = os.path.join(input_dir, filename)\n",
    "        result = md.convert(input_path)\n",
    "        output_filename = os.path.splitext(filename)[0] + \".md\"\n",
    "        output_path = os.path.join(output_dir, output_filename)\n",
    "        with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(result.text_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdebeb0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split MD\n",
    "\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "import pandas as pd\n",
    "\n",
    "md_dir = \"./files/md\"\n",
    "chunks_dir = \"./files/chunks\"\n",
    "os.makedirs(chunks_dir, exist_ok=True)\n",
    "\n",
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1500,\n",
    "    chunk_overlap=250,\n",
    ")\n",
    "\n",
    "for filename in os.listdir(md_dir):\n",
    "    if filename.lower().endswith(\".md\"):\n",
    "        file_path = os.path.join(md_dir, filename)\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            text = f.read()\n",
    "        chunks = splitter.split_text(text)\n",
    "        df = pd.DataFrame({\"chunk\": chunks})\n",
    "        output_csv = os.path.splitext(filename)[0] + \".csv\"\n",
    "        output_path = os.path.join(chunks_dir, output_csv)\n",
    "        df.to_csv(output_path, index=False, encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4bf4e95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Init Embedding Function\n",
    "\n",
    "from pymilvus import model\n",
    "\n",
    "gemini_ef = model.dense.GeminiEmbeddingFunction(\n",
    "    model_name=\"text-embedding-004\",\n",
    "    api_key=os.getenv(\"GOOGLE_API_KEY\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b8618f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embedding\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "\n",
    "chunks_dir = \"./files/chunks\"\n",
    "embeddings_dir = \"./files/embeddings\"\n",
    "os.makedirs(embeddings_dir, exist_ok=True)\n",
    "\n",
    "BATCH_SIZE = 100\n",
    "SLEEP_TIME = 1\n",
    "\n",
    "for filename in os.listdir(chunks_dir):\n",
    "    if filename.lower().endswith(\".csv\"):\n",
    "        file_path = os.path.join(chunks_dir, filename)\n",
    "        df = pd.read_csv(file_path)\n",
    "        document_name = os.path.splitext(filename)[0]\n",
    "        chunks = df[\"chunk\"].tolist()\n",
    "        all_embeddings = []\n",
    "        for i in range(0, len(chunks), BATCH_SIZE):\n",
    "            batch = chunks[i:i+BATCH_SIZE]\n",
    "            embeddings = gemini_ef.encode_documents(batch)\n",
    "            all_embeddings.extend([emb.tolist() for emb in embeddings])\n",
    "            time.sleep(SLEEP_TIME)\n",
    "        out_df = pd.DataFrame({\n",
    "            \"document_name\": [document_name] * len(df),\n",
    "            \"chunk\": df[\"chunk\"],\n",
    "            \"embeddings\": all_embeddings\n",
    "        })\n",
    "        output_path = os.path.join(\n",
    "            embeddings_dir, f\"{document_name}_embeddings.csv\")\n",
    "        out_df.to_csv(output_path, index=False, encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b53767fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymilvus import connections, FieldSchema, CollectionSchema, DataType, Collection\n",
    "\n",
    "connections.connect(\n",
    "    uri=\"https://\" + os.getenv(\"MILVUS_ENDPOINT\"),\n",
    "    token=os.getenv(\"MILVUS_API_KEY\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58a99afb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Schema\n",
    "\n",
    "fields = [\n",
    "    FieldSchema(name=\"id\", dtype=DataType.INT64, is_primary=True, auto_id=True),\n",
    "    FieldSchema(name=\"document_name\", dtype=DataType.VARCHAR, max_length=256, is_primary=False, auto_id=False),\n",
    "    FieldSchema(name=\"chunk\", dtype=DataType.VARCHAR, max_length=8192, is_primary=False, auto_id=False),\n",
    "    FieldSchema(name=\"embeddings\", dtype=DataType.FLOAT_VECTOR, dim=768, is_primary=False)\n",
    "]\n",
    "schema = CollectionSchema(fields, description=\"Document chunks and embeddings\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02dda4d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Collection\n",
    "\n",
    "from pymilvus import utility\n",
    "\n",
    "collection_name = \"llm_paper\"\n",
    "if collection_name not in utility.list_collections():\n",
    "    collection = Collection(name=collection_name, schema=schema)\n",
    "else:\n",
    "    collection = Collection(name=collection_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63bce356",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert embeddings\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "embeddings_dir = \"./files/embeddings\"\n",
    "for filename in os.listdir(embeddings_dir):\n",
    "    if filename.endswith(\"_embeddings.csv\"):\n",
    "        df = pd.read_csv(os.path.join(embeddings_dir, filename))\n",
    "        df[\"embeddings\"] = df[\"embeddings\"].apply(eval)\n",
    "        data = [\n",
    "            df[\"document_name\"].tolist(),\n",
    "            df[\"chunk\"].tolist(),\n",
    "            df[\"embeddings\"].tolist()\n",
    "        ]\n",
    "        collection.insert(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "259e6653",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Status(code=0, message=)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create index on embeddings column\n",
    "\n",
    "collection.create_index(\n",
    "    field_name=\"embeddings\",\n",
    "    index_params={\"index_type\": \"IVF_FLAT\", \"metric_type\": \"L2\", \"params\": {\"nlist\": 128}}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dd73e1fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start to install package: google-genai>=1.7.0\n",
      "successfully installed package: google-genai>=1.7.0\n",
      "Score: 0.6788\n",
      "Document: Zhang et al. - 2024 - Benchmarking the Text-to-SQL Capability of Large L\n",
      "Chunk: from scratch, with less effort. They typically use smarter decoding techniques (e.g., constraining the\n",
      "predictions of the decoder [39], or schema-aware denoising [50]) to prevent the production of invalid\n",
      "SQLs. RESDSQL [21] further decouples the intertwined process of schema linking (determining\n",
      "the schema items like tables and columns in a SQL) and skeleton parsing (determining the SQL\n",
      "keywords) , which alleviates the difficulty of Text-to-SQL. Apart from network architectures, the\n",
      "paradigms employed by these methods vary greatly in terms of input encoding, output decoding,\n",
      "neural training, output refinement, making Text-to-SQL a flourishing research area [17]. However,\n",
      "the highest accuracy on the Spider leaderboard achieved by traditional learning-based methods is\n",
      "79.9%, which is still far from being a reliable Text-to-SQL parser.\n",
      "\n",
      "2.2 LLM-based Text-to-SQL Methods\n",
      "---\n",
      "Score: 0.6998\n",
      "Document: Katsogiannis-Meimarakis and Koutrika - 2023 - A survey on deep learning approaches for text-to-S\n",
      "Chunk: 9 Conclusions\n",
      "\n",
      "The domain of text-to-SQL translation has received increas-\n",
      "ingly larger attention by both the database and NLP com-\n",
      "munities. The recent introduction of two large text-to-SQL\n",
      "datasets [107,112] has enabled the use of deep learning mod-\n",
      "els and spurred a new wave of innovation. To understand\n",
      "which milestones have been conquered and what obstacles\n",
      "lie ahead, it is necessary to provide a systematic and organ-\n",
      "ised study of the ﬁeld.\n",
      "\n",
      "This work explained the text-to-SQL problem and the\n",
      "available benchmarks, before diving into the systems. We\n",
      "provided a ﬁne-grained taxonomy of deep learning text-\n",
      "to-SQL systems, based on six axes: (a) schema linking,\n",
      "(b) natural language representation, (c) input encoding, (d)\n",
      "output decoding, (e) neural training, and (f ) output reﬁne-\n",
      "ment. For each axis of our taxonomy, we analysed all the\n",
      "approaches that have been presented so far and explained\n",
      "their strengths and weaknesses. We relied on this taxonomy\n",
      "to present some of the most important systems that have been\n",
      "proposed, grouping them together, in order to highlight their\n",
      "similarities, differences and innovations.\n",
      "\n",
      "Finally, having presented the current state of the art, we\n",
      "discussed open challenges and research opportunities that\n",
      "must be tackled in order to truly advance the ﬁeld of text-to-\n",
      "SQL, as well as broader challenges that are closely related\n",
      "to it. It is important to keep in mind, that the ultimate goal\n",
      "of text-to-SQL research is to empower the casual user to\n",
      "---\n",
      "Score: 0.7028\n",
      "Document: Zhang et al. - 2024 - Benchmarking the Text-to-SQL Capability of Large L\n",
      "Chunk: • For each task, we systematically assess the impact of information granularity on model\n",
      "performance and identify the optimal context learning strategies, such as zero-shot and\n",
      "few-shot, to maximize the performance of the models.\n",
      "\n",
      "2 Related Work\n",
      "\n",
      "2.1 Traditional Learning-based Text-to-SQL Methods\n",
      "\n",
      "Numerous traditional learning-based Text-to-SQL methods existed before the emergence of LLMs [60,\n",
      "52, 29]. These methods can be essentially divided into non-seq2seq and seq2seq methods according\n",
      "to their network architecture. In non-seq2seq methods, representative works [46, 2, 1, 15] typically\n",
      "employ a relation-aware self-attention mechanism as encoder to learn representations of questions and\n",
      "schemas, and then use a grammar-based decoder to generate the SQL as an abstract syntax tree [54],\n",
      "or utilize a sketch-based decoder to obtain the SQL via slot-filling [8, 16, 5]. These methods can\n",
      "further benefit from leveraging pre-trained language models like BERT and its extensions [7, 28, 55]\n",
      "for input embedding initialization. As another line of research, seq2seq methods [36, 26, 34, 23]\n",
      "directly translate NL questions into SQL queries through transformer-based [45] architectures in an\n",
      "end-to-end manner. These methods obtain competitive performance by fine-tuning rather than training\n",
      "\n",
      "2\n",
      "---\n",
      "Score: 0.7122\n",
      "Document: Zhang et al. - 2024 - Benchmarking the Text-to-SQL Capability of Large L\n",
      "Chunk: [15] Binyuan Hui, Ruiying Geng, Lihan Wang, Bowen Qin, Bowen Li, Jian Sun, and Yongbin Li. S2sql:\n",
      "Injecting syntax to question-schema interaction graph encoder for text-to-sql parsers. arXiv preprint\n",
      "arXiv:2203.06958, 2022.\n",
      "\n",
      "[16] Wonseok Hwang, Jinyeong Yim, Seunghyun Park, and Minjoon Seo. A comprehensive exploration on\n",
      "\n",
      "wikisql with table-aware word contextualization. arXiv preprint arXiv:1902.01069, 2019.\n",
      "\n",
      "[17] George Katsogiannis-Meimarakis and Georgia Koutrika. A survey on deep learning approaches for\n",
      "\n",
      "text-to-sql. The VLDB Journal, pages 1–32, 2023.\n",
      "\n",
      "[18] Yilun Kong, Jingqing Ruan, Yihong Chen, Bin Zhang, Tianpeng Bao, Shiwei Shi, Guoqing Du, Xiaoru\n",
      "Hu, Hangyu Mao, Ziyue Li, et al. Tptu-v2: Boosting task planning and tool usage of large language\n",
      "model-based agents in real-world systems. arXiv preprint arXiv:2311.11315, 2023.\n",
      "\n",
      "[19] Ayush Kumar, Parth Nagarkar, Prabhav Nalhe, and Sanjeev Vijayakumar. Deep learning driven natural\n",
      "\n",
      "languages text to sql query conversion: A survey. arXiv preprint arXiv:2208.04415, 2022.\n",
      "\n",
      "[20] Chia-Hsuan Lee, Oleksandr Polozov, and Matthew Richardson. Kaggledbqa: Realistic evaluation of\n",
      "\n",
      "text-to-sql parsers. arXiv preprint arXiv:2106.11455, 2021.\n",
      "\n",
      "[21] Haoyang Li, Jing Zhang, Cuiping Li, and Hong Chen. Resdsql: Decoupling schema linking and skeleton\n",
      "parsing for text-to-sql. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 37, pages\n",
      "13067–13075, 2023.\n",
      "---\n",
      "Score: 0.7160\n",
      "Document: Zhang et al. - 2024 - Benchmarking the Text-to-SQL Capability of Large L\n",
      "Chunk: (cid:66)\n",
      "\n",
      "Corresponding author: {hy.mao@pku.edu.cn, maohangyu@sensetime.com}\n",
      "∗These authors contributed equally.\n",
      "\n",
      "Preprint. Under review.\n",
      "\n",
      "\fFigure 1: Benchmarking tasks in Text-to-SQL pipeline.\n",
      "\n",
      "that play a crucial role in guiding LLMs to generate accurate SQL queries are yet to be determined.\n",
      "Although various design schemes are explored in different methods [13], there is no consensus on\n",
      "the most effective prompt template. Furthermore, the current benchmarks, while comprehensive\n",
      "in their assessment of end-to-end Text-to-SQL tasks, have not yet provided a detailed exploration\n",
      "of the models’ performance across the various sub-tasks and components of the Text-to-SQL pro-\n",
      "cess [32, 27]. A detailed exploration of these sub-tasks is crucial for a thorough evaluation of LLMs’\n",
      "cognitive capabilities and their role in facilitating the Text-to-SQL process. Therefore, it is necessary\n",
      "to develop a more granular benchmarking approach that can accurately reflect the multifaceted nature\n",
      "of Text-to-SQL and inform the creation of more effective LLM-based solutions.\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "# Query content\n",
    "\n",
    "from pymilvus import Collection, MilvusClient, model\n",
    "\n",
    "query_text = \"best practice for text2sql\"\n",
    "collection_name = \"llm_paper\"\n",
    "max_results = 5\n",
    "gemini_ef = model.dense.GeminiEmbeddingFunction(\n",
    "    model_name=\"text-embedding-004\",\n",
    "    api_key=os.getenv(\"GOOGLE_API_KEY\"),\n",
    ")\n",
    "query_embedding = gemini_ef.encode_documents([query_text])[0]\n",
    "\n",
    "client = MilvusClient(\n",
    "    uri=\"https://\" + os.getenv(\"MILVUS_ENDPOINT\"),\n",
    "    token=os.getenv(\"MILVUS_API_KEY\")\n",
    ")\n",
    "\n",
    "results = client.search(\n",
    "            collection_name=collection_name,\n",
    "            anns_field=\"embeddings\",\n",
    "            data=[query_embedding],\n",
    "            limit=max_results,\n",
    "            output_fields=[\"document_name\", \"chunk\"]\n",
    "        )\n",
    "\n",
    "for hits in results:\n",
    "    for hit in hits:\n",
    "        print(f\"Score: {hit.distance:.4f}\")\n",
    "        print(\"Document:\", hit.entity.get(\"document_name\"))\n",
    "        print(\"Chunk:\", hit.entity.get(\"chunk\"))\n",
    "        print(\"---\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
